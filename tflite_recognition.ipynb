{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "\n",
    "class GestureRecognizer:\n",
    "    def __init__(self, model_path=\"gesture_model.tflite\", labels_path=\"gesture_model_labels.json\"):\n",
    "        # Load the TFLite model and allocate tensors\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        self.interpreter.allocate_tensors()\n",
    "        \n",
    "        # Get input and output tensors info\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "        \n",
    "        # Load label encoder classes\n",
    "        with open(labels_path, 'r') as f:\n",
    "            self.classes = json.load(f)\n",
    "        \n",
    "        # Initialize MediaPipe Hands\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_drawing_styles = mp.solutions.drawing_styles\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=1,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "    def extract_landmarks(self, hand_landmarks):\n",
    "        \"\"\"Extract and normalize landmarks with scale invariance\"\"\"\n",
    "        landmarks = []\n",
    "        \n",
    "        # Get wrist landmark as origin\n",
    "        wrist = hand_landmarks.landmark[0]\n",
    "        wrist_x, wrist_y, wrist_z = wrist.x, wrist.y, wrist.z\n",
    "        \n",
    "        # Find the middle finger MCP joint (knuckle) for scale reference\n",
    "        middle_mcp = hand_landmarks.landmark[9]\n",
    "        \n",
    "        # Calculate hand scale as distance from wrist to middle finger MCP\n",
    "        scale = np.sqrt((middle_mcp.x - wrist_x)**2 + \n",
    "                        (middle_mcp.y - wrist_y)**2 + \n",
    "                        (middle_mcp.z - wrist_z)**2)\n",
    "        \n",
    "        # Process each landmark with scale normalization\n",
    "        for lm in hand_landmarks.landmark:\n",
    "            if scale > 0:\n",
    "                norm_x = (lm.x - wrist_x) / scale\n",
    "                norm_y = (lm.y - wrist_y) / scale\n",
    "                norm_z = (lm.z - wrist_z) / scale\n",
    "            else:\n",
    "                norm_x, norm_y, norm_z = 0, 0, 0\n",
    "            \n",
    "            landmarks.append({\n",
    "                \"x\": norm_x,\n",
    "                \"y\": norm_y,\n",
    "                \"z\": norm_z\n",
    "            })\n",
    "        \n",
    "        return landmarks\n",
    "\n",
    "    def predict_from_landmarks(self, landmarks):\n",
    "        \"\"\"Predict gesture from preprocessed landmarks using TFLite model\"\"\"\n",
    "        features = []\n",
    "        for lm in landmarks:\n",
    "            features.extend([lm['x'], lm['y'], lm['z']])\n",
    "        \n",
    "        # Convert to float32 and reshape\n",
    "        features = np.array(features, dtype=np.float32).reshape(1, -1)\n",
    "        \n",
    "        # Set input tensor\n",
    "        self.interpreter.set_tensor(self.input_details[0]['index'], features)\n",
    "        \n",
    "        # Run inference\n",
    "        self.interpreter.invoke()\n",
    "        \n",
    "        # Get output tensor\n",
    "        prediction = self.interpreter.get_tensor(self.output_details[0]['index'])[0]\n",
    "        \n",
    "        # Extract prediction info\n",
    "        predicted_class = int(np.argmax(prediction))\n",
    "        confidence = float(prediction[predicted_class])\n",
    "        predicted_label = self.classes[predicted_class]\n",
    "        \n",
    "        return predicted_label, confidence\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process a video frame and return gesture prediction\"\"\"\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.hands.process(rgb_frame)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                self.mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    hand_landmarks,\n",
    "                    self.mp_hands.HAND_CONNECTIONS,\n",
    "                    self.mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    self.mp_drawing_styles.get_default_hand_connections_style()\n",
    "                )\n",
    "                \n",
    "                landmarks = self.extract_landmarks(hand_landmarks)\n",
    "                label, confidence = self.predict_from_landmarks(landmarks)\n",
    "                \n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f\"{label}: {confidence:.2f}\",\n",
    "                    (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1,\n",
    "                    (0, 255, 0),\n",
    "                    2\n",
    "                )\n",
    "                \n",
    "                return frame, label, confidence\n",
    "        \n",
    "        return frame, None, None\n",
    "\n",
    "    def start_webcam(self):\n",
    "        \"\"\"Start webcam feed and recognize gestures in real-time\"\"\"\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Failed to read from webcam\")\n",
    "                break\n",
    "            \n",
    "            processed_frame, label, confidence = self.process_frame(frame)\n",
    "            cv2.imshow('Gesture Recognition', processed_frame)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
